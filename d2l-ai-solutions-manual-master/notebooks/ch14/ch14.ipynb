{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第14章 自然语言处理：预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 词嵌入（word2vec）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.1.1\n",
    "\n",
    "计算每个梯度的计算复杂度是多少？如果词表很大，会有什么问题呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.1.2\n",
    "\n",
    "英语中的一些固定短语由多个单词组成，例如“new york”。如何训练它们的词向量？提示:查看word2vec论文的第四节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.1.3\n",
    "\n",
    "让我们以跳元模型为例来思考word2vec设计。跳元模型中两个词向量的点积与余弦相似度之间有什么关系？对于语义相似的一对词，为什么它们的词向量（由跳元模型训练）的余弦相似度可能很高？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 近似训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.2.1\n",
    "\n",
    "如何在负采样中对噪声词进行采样？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.2.2\n",
    "\n",
    "验证式（14.24）是否有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.2.3\n",
    "\n",
    "如何分别使用负采样和分层softmax训练连续词袋模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 用于预训练词嵌入的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.3.1\n",
    "\n",
    "如果不使用下采样，本节中代码的运行时间会发生什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.3.2\n",
    "\n",
    "`RandomGenerator`类缓存`k`个随机采样结果。将`k`设置为其他值，看看它如何影响数据加载速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.3.3\n",
    "\n",
    "本节代码中的哪些其他超参数可能会影响数据加载速度？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 预训练word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.4.1\n",
    "\n",
    "使用训练好的模型，找出其他输入词在语义上相似的词。您能通过调优超参数来改进结果吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.4.2\n",
    "\n",
    "当训练语料库很大时，在更新模型参数时，我们经常对当前小批量的*中心词*进行上下文词和噪声词的采样。换言之，同一中心词在不同的训练迭代轮数可以有不同的上下文词或噪声词。这种方法的好处是什么？尝试实现这种训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 全局向量的词嵌入（GloVe）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.5.1\n",
    "\n",
    "如果词$w_i$和$w_j$在同一上下文窗口中同时出现，我们如何使用它们在文本序列中的距离来重新设计计算条件概率$p_{ij}$的方法？提示：参见GloVe论文的第4.2节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.5.2\n",
    "\n",
    "对于任何一个词，它的中心词偏置和上下文偏置在数学上是等价的吗？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 子词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.6.1\n",
    "\n",
    "例如，英语中大约有$3\\times 10^8$种可能的$6$-元组。子词太多会有什么问题呢？如何解决这个问题？提示:请参阅fastText论文第3.2节末尾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.6.2\n",
    "\n",
    "如何在连续词袋模型的基础上设计一个子词嵌入模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.6.3\n",
    "\n",
    "要获得大小为$m$的词表，当初始符号词表大小为$n$时，需要多少合并操作？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.6.4\n",
    "\n",
    "如何扩展字节对编码的思想来提取短语？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7 词的相似度和类比任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.7.1\n",
    "\n",
    "使用`TokenEmbedding('wiki.en')`测试fastText结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.7.2\n",
    "\n",
    "当词表非常大时，我们怎样才能更快地找到相似的词或完成一个词的类比呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.8 来自Transformer的双向编码器表示（BERT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.8.1\n",
    "\n",
    "为什么BERT成功了？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同期的模型ELMo对上下文进行双向编码，解决了一词多意等问题，但是智能使用特定于某个任务的架构，不能进行finetune；同期的GPT是任务无关性的，但是仅能从做导游对上下文进行编码。而BERT结合了二者的优势，对上下文进行双向编码，且任务无关，对大多数nlp任务只需要对架构进行finetune即可。故BERT在同时期使用效果更好！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.8.2\n",
    "\n",
    "在所有其他条件相同的情况下，掩蔽语言模型比从左到右的语言模型需要更多或更少的预训练步骤来收敛吗？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   在其他条件相同的情况下，掩蔽语言模型需要更多的预训练步骤来收敛。这是因为掩蔽语言模型需要预测缺失的'mask'词汇，需要更高的上下文理解能力和更强的推理能力。而从左到右的语言模型只需要预测下一个词汇，相对而言更简单。\n",
    "  \n",
    "   举个具体的例子：在BERT为提高上下文理解能力与推理能力所作的掩敝语言模型mlm中，选择15%的词元作为遮盖词元，且为了使后续不同下游任务能够微调，又进行了80%的时间将原始词元替换为mask；10%的时间设置为原所有文本中的随机词元；10%的时间保持原来的词元不变。上述操作增加了模型的复杂程度，所以迭代步数要更多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.8.3\n",
    "\n",
    "在BERT的原始实现中，`BERTEncoder`中的位置前馈网络（通过`d2l.EncoderBlock`）和`MaskLM`中的全连接层都使用高斯误差线性单元（Gaussian error linear unit，GELU）作为激活函数。研究GELU与ReLU之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU是一种激活函数，与ReLU相比，它的主要差异在于它的形状更加平滑。ReLU在输入为负数时输出为0，而GELU在输入为负数时输出一个小的非零值，这使得GELU更加平滑。此外，GELU还具有一些其他的优势，例如在处理较大的数据集时具有更好的性能，以及对于一些复杂的神经网络模型，GELU能够提供更好的训练效果。总体来说，GELU是一种更加灵活和高效的激活函数，可以在许多不同的应用场景中发挥作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.9.1\n",
    "\n",
    "为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：`pip install nltk`。在代码中先`import nltk`。然后下载Punkt语句词元分析器：`nltk.download('punkt')`。要拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用`nltk.tokenize.sent_tokenize(sentences)`将返回两个句子字符串的列表：`['This is great !', 'Why not ?']`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把shuffle去了好直观对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def _read_wiki2(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # ⼤写字⺟转换为⼩写字⺟\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                    for line in lines if len(line.split(' . ')) >= 2]\n",
    "#     random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "para_ori = _read_wiki2(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见原本是通过' . ' 来分割的，注意必须是dot两边都有一个空格才能分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " ['it met with positive sales in japan , and was praised by both japanese and western critics',\n",
       "  'after release , it received downloadable content , along with an expanded edition in november of that year',\n",
       "  'it was also adapted into manga and an original video animation series',\n",
       "  \"due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014\",\n",
       "  'media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4 .'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_ori[2]), para_ori[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting nltk\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 744.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\envs\\pytorch\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (e:\\anaconda\\envs\\pytorch\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个if判断条件是为了消去一些空字符行用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xiao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " ['it met with positive sales in japan , and was praised by both japanese and western critics .',\n",
       "  'after release , it received downloadable content , along with an expanded edition in november of that year .',\n",
       "  'it was also adapted into manga and an original video animation series .',\n",
       "  \"due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 .\",\n",
       "  'media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4 .'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def _read_wiki_nltk(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # ⼤写字⺟转换为⼩写字⺟\n",
    "    paragraphs = [nltk.tokenize.sent_tokenize(line.strip().lower())\n",
    "                    for line in lines if len(line.split(' . ')) >= 2]\n",
    "#     random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "para_nltk = _read_wiki_nltk(data_dir)\n",
    "len(para_nltk[2]), para_nltk[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = nltk.tokenize.sent_tokenize(para_nltk[0])\n",
    "# len(demo), demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见nltk的作用体现了。区分二者可以人为再构造几个不同的标点符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This is great !', 'Why not ?', 'ah!a'], ['this is great ! why not ? ah!a'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lined = 'This is great ! Why not ? ah!a'\n",
    "nltk.tokenize.sent_tokenize(lined), lined.strip().lower().split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.9.2\n",
    "\n",
    "如果我们不过滤出一些不常见的词元，词量会有多大？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将class _WikiTextDataset中的vocab中的min_freq=0即可，输出len(vocab)=28886"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.10 预训练BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.10.1\n",
    "\n",
    "在实验中，我们可以看到遮蔽语言模型损失明显高于下一句预测损失。为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩蔽语言模型是一种预测中间掩盖掉的词的任务，相当于完形填空：它要求模型在给定上下文的一些词的情况下，预测中间扣除的词是什么。相比之下，下一句预测任务是要求模型在给定一句话的情况下，预测下一句话是什么。\n",
    "掩蔽语言模型的损失会明显大于下一句预测任务的损失，是因为掩蔽语言模型需要预测多个位置的单词，而下一句预测任务只需要预测一整句话，因此掩蔽语言模型需要更多的预测任务，模型需要更好地理解句子中的语言结构和上下文关系，才能准确地预测每个位置的单词，因此其损失会更大。\n",
    "具体到本节中的例子而言，对下一句预测仅仅抽取了原句子中'cls'词元对应的hiddens信息，送入mlp的输入为(bs,1,h);而在mlm中，送入mlp的是(bs,Tmask,h),输出相应的hat值(bs,Tmask,len(vocab)),其中Tmask就是每个句子中需要预测的词元数量，书上表示的是10(模型更加复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习14.10.2\n",
    "\n",
    "将BERT输入序列的最大长度设置为512（与原始BERT模型相同）。使用原始BERT模型的配置，如$\\text{BERT}_{\\text{LARGE}}$。运行此部分时是否遇到错误？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笔者使用的4块2080ti，更改后会报OutOfMemoryError错误，是因为一次性读取的数据超出内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
